{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Neural Networks\n",
    "\n",
    "\n",
    "This assignment is due on Moodle by **11:59pm on Friday October 25**. \n",
    "Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.\n",
    "Your solutions to computational questions should include any specified Python code and results \n",
    "as well as written commentary on your conclusions.\n",
    "Remember that you are encouraged to discuss the problems with your instructors and classmates, \n",
    "but **you must write all code and solutions on your own**. For a refresher on the course **Collaboration Policy** click [here](https://github.com/BoulderDS/CSCI5622-Machine-Learning/blob/master/info/syllabus.md#collaboration-policy).\n",
    "\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda (Version: 2019.07) with Python 3.7. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. \n",
    "- In this homework, we will use [Keras](https://keras.io/) to implement a classifier. First upgrade your `pip` package manager to the latest version (Version >19.0). Then install the current stable release for CPU-only of [TensorFlow](https://www.tensorflow.org/install) (version: 2.0.0) as the backend for Keras. We will use [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) as our Keras API.\n",
    "```\n",
    "pip install --upgrade pip\n",
    "pip install tensorflow\n",
    "```\n",
    "**Acknowledgment** : Chris Ketelsen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[25 points] Problem 1 - Single-Layer and Multilayer Perceptron Learning\n",
    "---\n",
    "\n",
    "**Part 1 [15 points]:** Consider learning the following concepts with either a single-layer or multilayer perceptron where all hidden and output neurons utilize the *indicator* activation functions. For each of the following concepts, state whether the concept can be learned by a single-layer perceptron. Briefly justify your response by providing weights, biases, and the *indicator* activation functions if applicable:\n",
    "\n",
    "- $\\; \\texttt{NOT } x_1$\n",
    "\n",
    "- $\\; x_1 \\texttt{ NAND } x_2$\n",
    "\n",
    "- $\\; x_1 \\texttt{ XNOR } x_2$ (output 1 when $x_1 = x_2$ and 0 otherwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c85384be6b7c21b60e806f68a49d0a2d",
     "grade": true,
     "grade_id": "cell-5504bb80827fe61b",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 [10 points]:** Determine an architecture and specific values of the weights and biases in a single-layer or multilayer perceptron with *indicator* activation functions that can learn $x_1 \\texttt{ XNOR } x_2$. Make a truth table of $x_1$, $x_2$, and $x_1 \\texttt{ XNOR } x_2$, describe your perceptron's architecture, and state your weight matrices and bias vectors in Markdown below. Then demonstrate that your solution is correct by implementing forward propagation for your network in Python and showing that it produces the correct boolean output values for each of the four possible combinations of $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3dd94f21437a23eceec230c4f66dd863",
     "grade": true,
     "grade_id": "cell-fd1e475a5ef92def",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a95bc02f140db70f57390569aeb61fa2",
     "grade": true,
     "grade_id": "cell-330996afb5a81650",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[25 points] Problem 2 - Back propagation\n",
    "---\n",
    "\n",
    "In this problem you will gain some intuition about why training deep neural networks can be very time consuming.  Consider training a chain-like neural network: \n",
    "\n",
    "![chain-like nn](figs/chain_net.png)\n",
    "\n",
    "Note that this network has three weights $W^1, W^2, W^3$ and three biases $b^1, b^2,$ and $b^3$ (for this problem you can think of each parameter as a single value or as a $1 \\times 1$ matrix). Suppose that each hidden and output neuron is equipped with a sigmoid activation function and the loss function is given by \n",
    "\n",
    "$$\n",
    "\\ell(y, a^4) = \\frac{1}{2}(y - a^4)^2  \n",
    "$$\n",
    "\n",
    "where $a^4$ is the value of the activation at the output neuron and $y \\in \\{0,1\\}$ is the true label associated with the training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 [5 points]:** Suppose each of the weights is initialized to $W^k = 1.0$ and each bias is initialized to $b^k = -0.5$.  Use forward propagation to find the activities and activations associated with each hidden and output neuron for the training example $(x, y) = (0.5,0)$. Show your work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ed8ae08caeac509ef436f5d18c8223c",
     "grade": true,
     "grade_id": "cell-6512c42fc5e9ce1b",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 [5 points]:** Use Back-Propagation to compute the weight and bias derivatives $\\partial \\ell / \\partial W^k$ and $\\partial \\ell / \\partial b^k$ for $k=1, 2, 3$.  Show all work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d95b2a0dc16f45336d915205d119763a",
     "grade": true,
     "grade_id": "cell-6a3b895fa888e925",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3 [5 points]:** Implement the following activation functions:\n",
    "* ReLU\n",
    "* Sigmoid\n",
    "* softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3095d722bd4a8f2fdf5aaca330d63aa",
     "grade": false,
     "grade_id": "cell-3b7e3adaffe2c2ee",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def sigmoid(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def softmax(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4806e7d6616bd4841cbaa7caececbae8",
     "grade": true,
     "grade_id": "cell-0bded3752d3226b9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# for grading - ignore\n",
    "assert relu(5) == 5\n",
    "assert relu(-5) == 0\n",
    "assert relu(0) == 0\n",
    "assert sigmoid(0.458) == 0.61253961344091512\n",
    "assert sigmoid(2) == 0.8807970779778823\n",
    "res = softmax([1,2,4])\n",
    "temp = [0.04201007, 0.1141952 , 0.84379473]\n",
    "for i in range(len(temp)):\n",
    "    assert res[i] - temp[i] < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 [5 points]:** Implement the following Loss functions:\n",
    "* mean squared error\n",
    "* mean absolute error\n",
    "* hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82e56ad803e5c2c632c83373c33c163e",
     "grade": false,
     "grade_id": "cell-84456d343bfeca31",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(yhat, y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "def mean_absolute_error(yhat, y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def hinge(yhat, y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0f9814d8d7a347301d5d7fe6418b037",
     "grade": true,
     "grade_id": "cell-550e920d814cb6d3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# for grading - ignore\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "assert mean_squared_error(y_pred,y_true) == 0.375\n",
    "assert mean_absolute_error(y_pred,y_true) == 0.5\n",
    "assert hinge(y_pred,y_true) == 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 5 [5 points]:** Explain the vanishing gradient problem. When would you observe this? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "550c7233d3dc395aa4965b23ce1126a4",
     "grade": true,
     "grade_id": "cell-59ccf91056b5d8bb",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[25 Points] Problem 3 - Build a feedforward neural network\n",
    "---\n",
    "\n",
    "In this problem you will implement a class representing a general feed-forward neural network that utilizes the sigmoid activation functions. Your tasks will be to implement forward propagation, prediction, back propagation, and a general train routine to learn the weights in your network via stochastic gradient descent.\n",
    "\n",
    "The skeleton for the network class is below. Note that this class is almost identical to the one you worked with in the \"hands-on neural network\" in-class notebook, so you should look at there to remind yourself of the details. Scroll down to find more information about your tasks as well as unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c062beefe19f25b4a7e2a687b76c6107",
     "grade": false,
     "grade_id": "cell-b8abc0ac570aac74",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, sizes, keep_prob=-1):\n",
    "        self.L = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(n, 1) for n in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(n, m) for (\n",
    "            m, n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        self.keep_prob = keep_prob\n",
    "        self.acc_train_array = []\n",
    "        self.acc_test_array = []\n",
    "\n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        activation function\n",
    "        \"\"\"\n",
    "        return sigmoid(z)\n",
    "\n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of activation function\n",
    "        \"\"\"\n",
    "        return sigmoid_prime(z)\n",
    "\n",
    "    def forward_prop(self, a):\n",
    "        \"\"\"\n",
    "        memory aware forward propagation for testing\n",
    "        only.  back_prop implements it's own forward_prop\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def grad_cost(self, a, y):\n",
    "        \"\"\"\n",
    "        gradient of cost function\n",
    "        Assumes C(a,y) = (a-y)^2/2\n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "\n",
    "    def SGD_train(self, train, epochs, eta, lam=0.0, verbose=True, test=None):\n",
    "        \"\"\"\n",
    "        SGD for training parameters\n",
    "        epochs is the number of epocs to run\n",
    "        eta is the learning rate\n",
    "        lam is the regularization parameter\n",
    "        If verbose is set will print progressive accuracy updates\n",
    "        If test set is provided, routine will print accuracy on test set as learning evolves\n",
    "        \"\"\"\n",
    "        n_train = len(train)\n",
    "        for epoch in range(epochs):\n",
    "            perm = np.random.permutation(n_train)\n",
    "            for kk in range(n_train):\n",
    "                xk = train[perm[kk]][0]\n",
    "                yk = train[perm[kk]][1]\n",
    "                # TODO: get gradients with xk, yk and do SGD on weights and biases\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "            if verbose:\n",
    "                if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "                    acc_train = self.evaluate(train)\n",
    "                    self.acc_train_array.append(acc_train)\n",
    "                    if test is not None:\n",
    "                        acc_test = self.evaluate(test)\n",
    "                        self.acc_test_array.append(acc_test)\n",
    "                        print(\"Epoch {:4d}: Train {:10.5f}, Test {:10.5f}\".format(\n",
    "                            epoch+1, acc_train, acc_test))\n",
    "                    else:\n",
    "                        print(\"Epoch {:4d}: Train {:10.5f}\".format(\n",
    "                            epoch+1, acc_train))\n",
    "\n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation for derivatives of C wrt parameters\n",
    "        \"\"\"\n",
    "        db_list = [np.zeros(b.shape) for b in self.biases]\n",
    "        dW_list = [np.zeros(W.shape) for W in self.weights]\n",
    "        \n",
    "        a = x\n",
    "        a_list = [a]\n",
    "        z_list = [np.zeros(a.shape)]  # Pad with a placeholder so that indices match\n",
    "\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(W, a) + b\n",
    "            z_list.append(z)\n",
    "            a = self.g(z)\n",
    "            a_list.append(a)\n",
    "\n",
    "        # Back propagate deltas to compute derivatives\n",
    "        # The following list gives hints on how to do it\n",
    "        # calculating delta (Error) for the output layer\n",
    "        # for the appropriate layers compute db_list[ell], dW_list[ell], delta\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return (dW_list, db_list)\n",
    "    \n",
    "    def back_prop_dropout(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation with dropout on the hidden layers other than the output layer.\n",
    "        \n",
    "        Dropout layer can be thought of as a special linear layer between layers.\n",
    "        \"\"\"\n",
    "        db_list = [np.zeros(b.shape) for b in self.biases]\n",
    "        dW_list = [np.zeros(W.shape) for W in self.weights]\n",
    "        \n",
    "        a = x\n",
    "        a_list = [a]\n",
    "        z_list = [np.zeros(a.shape)]  # Pad with a placeholder so that indices match\n",
    "        # TODO: implement dropout using self.keep_prob\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return (dW_list, db_list)\n",
    "\n",
    "    def evaluate(self, test):\n",
    "        \"\"\"\n",
    "        Evaluate current model on labeled test data\n",
    "        \"\"\"\n",
    "        ctr = 0\n",
    "        for x, y in test:\n",
    "            yhat = self.forward_prop(x)\n",
    "            ctr += np.argmax(yhat) == np.argmax(y)\n",
    "        return float(ctr) / float(len(test))\n",
    "\n",
    "\n",
    "def sigmoid(z, threshold=20):\n",
    "    z = np.clip(z, -threshold, threshold)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "\n",
    "def mnist_digit_show(flatimage, outname=None):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    image = np.reshape(flatimage, (-1, 14))\n",
    "\n",
    "    plt.matshow(image, cmap=plt.cm.binary)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if outname:\n",
    "        plt.savefig(outname)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 [15 points]:** Implement `SGD_train`, `back_prop`, and `forward_prop`. Use the following test cases to verify if the code is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d6d5f4292a5c37a83b510f0084a5920",
     "grade": true,
     "grade_id": "cell-7632a78793a2588e",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite('prob 3', Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 [10 points]:**\n",
    "\n",
    "Run the above Network on MNIST Dataset and report the following (feel free to experiment with different learning rates).\n",
    "\n",
    "* Change the hidden layer dimensions and experiment with these values: [5, 10, 20].\n",
    "* Plot accuracies of different hidden layer dimensions vs. epochs for both training and testing.\n",
    "* Explain the effect of hidden layer dimension on performance. \n",
    "\n",
    "**Note:** Accuracies are stored in `self.acc_train_array` and `self.acc_test_array` if `verbose` is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './data/tinyMNIST.pkl.gz'\n",
    "f = gzip.open(location, 'rb')\n",
    "u = pickle._Unpickler(f)\n",
    "u.encoding = 'latin1'\n",
    "train, test = u.load()\n",
    "input_dimensions = len(train[0][0])\n",
    "output_dimensions = len(train[0][1])\n",
    "print('Number of Input Features: ', input_dimensions)\n",
    "print('Number of Output classes: ', output_dimensions)\n",
    "\n",
    "nns = []\n",
    "for hidden_layer_dimensions in [5, 10, 20]:\n",
    "    print('\\nHidden Layer Dimensions: ', hidden_layer_dimensions)\n",
    "    nn = Network([input_dimensions, hidden_layer_dimensions, output_dimensions])\n",
    "    nns.append(nn)\n",
    "    nn.SGD_train(train, epochs=200, eta=0.1, lam=0.0001, verbose=True, test=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3fb22c66cc5fec6b672b78f087f60cf",
     "grade": true,
     "grade_id": "cell-5e86a6c1f3a99b62",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot testing results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "427d9f5cd107c0d68546b27a7b94595a",
     "grade": true,
     "grade_id": "cell-9f1f5c4e9767033a",
     "locked": false,
     "points": 2.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give your explanation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd36ca1bb36eb7f7132e0538a571aac0",
     "grade": true,
     "grade_id": "cell-fb5c5d0fd9a6d900",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Credit [10 points]:** Implement dropout by filling the `back_prop_dropout` function and update the `SGD_train` function to use it. Explain the impact of dropout on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_dimensions = 60\n",
    "nn = Network([input_dimensions, hidden_layer_dimensions, output_dimensions], keep_prob=0.5)\n",
    "nn.SGD_train(train, epochs=400, eta=0.1, lam=0, verbose=True, test=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1f81c6e069ad24bed4034816ccc6c65",
     "grade": true,
     "grade_id": "cell-f7345360536d4c03",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[25 Points] Problem 4 - Implement RNN Network to classify whether text is spam or ham \n",
    "---\n",
    "\n",
    "Dataset is obtained from UCI Machine Learning repository consisting of SMS tagged messages (labelled as either **ham** (legitimate) or **spam**) that have been collected for SMS Spam research.\n",
    "\n",
    "We will now use [Keras](https://keras.io/) to implement a classifier. First upgrade your `pip` package manager to the latest version (Version >19.0). Then install the current stable release for CPU-only of [TensorFlow](https://www.tensorflow.org/install) (version: 2.0.0) as the backend for Keras. We will use [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) as our Keras API.\n",
    "\n",
    "Update the snippet below to build a Sequential model with an embedding layer, and an LSTM layer, and a dense layer. This question allows you to get familiar with popular deep learning toolkits and the solution only has a few lines. In practice, there is no need to reinvent the wheels.\n",
    "\n",
    "\n",
    "Learn more about RNN : https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 [15 points]:** Complete the following functions `init`, `train`, and `evaluate` functions and report the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2787ad7868837a8c5324bdce37a22c90",
     "grade": false,
     "grade_id": "cell-e5faa2bc69b4d8cf",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    '''\n",
    "    RNN classifier\n",
    "    '''\n",
    "\n",
    "    def __init__(self, train_x, train_y, test_x, test_y, dict_size=5000,\n",
    "                 example_length=150, embedding_length=32, epoches=5, batch_size=128):\n",
    "        '''\n",
    "        initialize RNN model\n",
    "        :param train_x: training data\n",
    "        :param train_y: training label\n",
    "        :param test_x: test data\n",
    "        :param test_y: test label\n",
    "        :param epoches: number of ephoches to run\n",
    "        :param batch_size: batch size in training\n",
    "        :param embedding_length: size of word embedding\n",
    "        :param example_length: length of examples\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.epoches = epoches\n",
    "        self.example_len = example_length\n",
    "        self.dict_size = dict_size\n",
    "        self.embedding_len = embedding_length\n",
    "\n",
    "        # preprocess training data\n",
    "        tok = Tokenizer(num_words=dict_size)\n",
    "        tok.fit_on_texts(train_x)\n",
    "        sequences = tok.texts_to_sequences(train_x)\n",
    "        self.train_x = sequence.pad_sequences(\n",
    "            sequences, maxlen=self.example_len)\n",
    "        sequences = tok.texts_to_sequences(test_x)\n",
    "        self.test_x = sequence.pad_sequences(\n",
    "            sequences, maxlen=self.example_len)\n",
    "\n",
    "        self.train_y = train_y\n",
    "        self.test_y = test_y\n",
    "\n",
    "        # TODO: build model with Embedding, LSTM and dense layers.\n",
    "        # Please refer to Sequence classification with LSTM : \n",
    "        #     https://keras.io/getting-started/sequential-model-guide/#examples\n",
    "        # Documentation for LSTM layer in : \n",
    "        #     https://keras.io/layers/recurrent/#lstm\n",
    "        self.model = Sequential()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                           optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    def train(self, verbose=0):\n",
    "        '''\n",
    "        fit in data and train model\n",
    "        please refer to the fit method in https://keras.io/models/model/#fit\n",
    "        make sure you use batchsize and epochs appropriately.\n",
    "        :return: None\n",
    "        '''\n",
    "        # TODO: fit in data to train your model\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def evaluate(self):\n",
    "        '''\n",
    "        evaluate trained model\n",
    "        please refer to the evaluate method in https://keras.io/models/model/#evaluate\n",
    "        :return: [loss, accuracy]\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_data(location):\n",
    "    return pickle.load(open(location,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = load_data('./data/spam_data.pkl')\n",
    "rnn = RNN(train_x, train_y, test_x, test_y, epoches=5)\n",
    "rnn.train(verbose=1)\n",
    "loss, accuracy = rnn.evaluate()\n",
    "print('Accuracy for LSTM: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ccf7f440d3189f35e9610eacb0c3bec",
     "grade": true,
     "grade_id": "cell-41f4dc66c60e24b2",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 [10 points]:** \n",
    "* Change the embedding length and experiment with these values: [8, 16, 32, 48, 64].\n",
    "* Plot training accuracies of different embedding lengths vs. epochs.\n",
    "* Observe and explain the impact of embedding length in LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNNs = []\n",
    "test_accuracy_array = []\n",
    "for embedding_len in [8, 16, 32, 48, 64]:\n",
    "    train_x, test_x, train_y, test_y = load_data('./data/spam_data.pkl')\n",
    "    rnn = RNN(train_x, train_y, test_x, test_y, epoches=5, embedding_length=embedding_len)\n",
    "    RNNs.append(rnn)\n",
    "    rnn.train(verbose=1)\n",
    "    loss, accuracy = rnn.evaluate()\n",
    "    test_accuracy_array.append(accuracy)\n",
    "    print('Accuracy for LSTM: ', accuracy)\n",
    "\n",
    "train_accuracy_matrix = np.array(list(map(lambda x: x.model.history.history[\"accuracy\"], RNNs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f95710664720b2757c89ccef177b592",
     "grade": true,
     "grade_id": "cell-bbb3e96b84261447",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give your observation and explanation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c47bfe1f75ebab584da5358020107ed",
     "grade": true,
     "grade_id": "cell-b621817b4445c626",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional survey.\n",
    "***\n",
    "\n",
    "We are always interested in your feedback. At the end of each homework, there is a simple anonymous feedback [survey](https://forms.gle/6Kf72C26am1SAjtg6) to solicit your feedback for how to improve the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
