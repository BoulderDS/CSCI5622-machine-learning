{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On KNN and Perceptron\n",
    "***\n",
    "\n",
    "In this notebook we'll investigate Scikit-Learn's implementation of K-Nearest Neighbors and the Perceptron classifier.  In addition, we'll look at how we can evaluate the performance our classifiers with a so-called confusion matrix.  \n",
    "\n",
    "**Note**: There are some helper functions at the bottom of this notebook.  Scroll down and execute those cells before continuing. \n",
    "\n",
    "**Acknowledgment**: Chris Ketelsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Classifying Iris Species \n",
    "***\n",
    "\n",
    "In this problem we'll use K-Nearest Neighbors to classify species of irises based on certain physical characteristics.  The so-called [_iris dataset_](https://en.wikipedia.org/wiki/Iris_flower_data_set) is a popular dataset for prototyping classification algorithms. We can load the iris dataset from Scikit-Learn directly. The dataset contains four features: sepal length, sepal width, pedal length, and pedal width and three classes defined by the species of iris: setosa, versicolor, and virginica. We'll only use the sepal dimensions so that we can easily visualize the data. \n",
    "\n",
    "Execute the following code cell to load training and validation sets for the iris data set and then plot the data.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid, target_names = load_iris()\n",
    "print(\"classes = \", target_names)\n",
    "plot_iris(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: How many examples are in the training set?  How many examples belong to each of the three classes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Next we'll train a KNN classifier to predict iris species based on the sepal measurement features.  The KNN classifier in Scikit-Learn is called [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).  Go now and check out the documentation. Define and fit a model with $K=15$ to the training set.  The `plot_knn_boundary` function will then plot the KNN decision boundary against the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = # TODO \n",
    "plot_knn_boundary(X_train, y_train, knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Play with the value of $K$ above.  How does the character of the decision boundary change with $K$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Until this point we've been plotting the KNN decision boundary against the training data, but really we're interested in how our model does on the validation set.  The following code will train a 1-NN classifier and plot the decision boundary against the validation data. How many points in total are misclassified?  Which species get confused with each other the most? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(1).fit(X_train, y_train)\n",
    "plot_knn_boundary(X_valid, y_valid, knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Counting misclassified points becomes much more difficult when our data sets are very large.  One convenient method for analyzing misclassification is by constructing the so-called confusion matrix. The confusion matrix is `(# classes)` $\\times$ `(# classes)` matrix such that the entry $C_{ij}$ is the number of examples with _true_ label $i$ predicted to have label $j$. \n",
    "\n",
    "We can compute the confusion matrix using Scikit-Learn's [confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function. Read the documentation and then fill in the missing code to compute the confusion matrix for the validation data and the 1-NN classifier.  Do the entries in $C$ agree (roughly) with the visual counts you made above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_hat_valid = # TODO \n",
    "C = # TODO \n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**: Vary the number of nearest neighbors used in KNN above and recompute the confusion matrix.  Describe your results. Does there seem to be a particular setting that works better than the others for the validation data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part G**: Fill in the code below to compute the error rate on the validation data from the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(C):\n",
    "    # TODO \n",
    "    return 0 \n",
    "\n",
    "print(\"error rate = {}\".format(error_rate(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: The Perceptron on Simulated Data\n",
    "***\n",
    "\n",
    "In this problem you'll fit a perceptron model to linearly separable data as well as mildly not-linearly separable data. Execute the following cell to load and plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = poly_data(100, sep=0.05, rot=np.pi/6)\n",
    "fig, ax = data_plot(scatter=[(X, y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Our first task will be to fit a perceptron model to the data using Scikit-Learn's [Perceptron](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) classifier.  Go now and look at the documentation. Then fit a model using only a single pass of the Perceptron Algorithm. What values for the weights and the bias did the algorithm find? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perc = Perceptron(max_iter=1, alpha=0.0, shuffle=False)\n",
    "perc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Plot the resulting perceptron decision boundary by filling in the below. Was the perceptron able to perfectly fit the training data with just a single epoch? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = data_plot(scatter=[(X, y)])\n",
    "xplot = np.linspace(-1.5, 1.5, 20)\n",
    "w, b = perc.coef_[0], perc.intercept_[0]\n",
    "yplot = # TODO \n",
    "ax.plot(xplot, yplot, lw=3, color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Next we're going to augment the data set so that it's no longer linearly separable.  Execute the following cell to see the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = poly_data(100, sep=0.05, rot=np.pi/6)\n",
    "X_new = np.concatenate((X, np.array([[0.5, 0.5]])))\n",
    "y_new = np.concatenate((y, np.array([-1])))\n",
    "fig, ax = data_plot(scatter=[(X_new, y_new)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Fit a new perceptron classifier to the data using the same parameters as before.  It's very important that you keep the `shuffle=False` flag because we want the new blue point to be the last point encountered in the epoch.  Plot the new decision boundary.  How badly did the new rogue point disrupt the classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_perc = Perceptron(max_iter=1, alpha=0.0, shuffle=False)\n",
    "new_perc.fit(X_new, y_new)\n",
    "xplot = np.linspace(-1.5, 1.5, 20)\n",
    "w, b = new_perc.coef_[0], perc.intercept_[0]\n",
    "yplot = # TODO \n",
    "fig, ax = data_plot(scatter=[(X_new, y_new)])\n",
    "ax.plot(xplot, yplot, lw=3, color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Adjust some of the parameters and settings above to see if you can obtain a better decision boundary.  Things that might be helpful to change are the number of epochs and whether the data is shuffled between each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Functions for KNN and iris dataset\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def load_iris(standardize=False, random_state=1234): \n",
    "    \n",
    "    from sklearn import datasets\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Load the data and grab first two features \n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data[:,:2], iris.target \n",
    "        \n",
    "    # Randomly split into validation and training sets \n",
    "    ones = np.ones(50, dtype=int)\n",
    "    valid_mask = np.full(50, False)\n",
    "    valid_mask[np.random.choice(range(50), replace=False, size=16)] = True \n",
    "    train_mask = np.logical_not(valid_mask)\n",
    "    X_train = np.concatenate((X[y==0][train_mask], X[y==1][train_mask], X[y==2][train_mask]))\n",
    "    y_train = np.concatenate((0 * ones[train_mask], 1 * ones[train_mask], 2 * ones[train_mask]))\n",
    "    X_valid = np.concatenate((X[y==0][valid_mask], X[y==1][valid_mask], X[y==2][valid_mask]))\n",
    "    y_valid = np.concatenate((0 * ones[valid_mask], 1 * ones[valid_mask], 2 * ones[valid_mask]))\n",
    "    \n",
    "    # Standardize data if desired \n",
    "    if standardize: \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_valid = scaler.transform(X_valid)\n",
    "        \n",
    "    return X_train, y_train, X_valid, y_valid, iris.target_names\n",
    "    \n",
    "\n",
    "def plot_iris(X, y):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "    name_color_dict = {\n",
    "        0: (\"steelblue\", \"setosa\"),\n",
    "        1:(\"#a76c6e\", \"versicolor\"),\n",
    "        2:(\"#6a9373\", \"virginica\")\n",
    "    }\n",
    "    for k in [0,1,2]:\n",
    "        ax.scatter(X[y==k, 0], X[y==k, 1], color=name_color_dict[k][0],\n",
    "                   s=100, label=name_color_dict[k][1])\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.legend(loc=\"upper right\", fontsize=16)\n",
    "    ax.set_xlabel(\"sepal length (cm)\", fontsize=16)\n",
    "    ax.set_ylabel(\"sepal width (cm)\", fontsize=16)\n",
    "    \n",
    "def plot_knn_boundary(X, y, model):\n",
    "    \n",
    "    from matplotlib import colors\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "    \n",
    "    name_color_dict = {\n",
    "        0: (\"steelblue\", \"setosa\"),\n",
    "        1:(\"#a76c6e\", \"versicolor\"),\n",
    "        2:(\"#6a9373\", \"virginica\")\n",
    "    }\n",
    "    for k in [0,1,2]:\n",
    "        ax.scatter(X[y==k, 0], X[y==k, 1], color=name_color_dict[k][0],\n",
    "                   s=100, label=name_color_dict[k][1], edgecolors=\"white\", zorder=2)\n",
    "    \n",
    "    # Plot the decision boundary. \n",
    "    x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n",
    "    y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.025), np.arange(y_min, y_max, 0.025))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Define custom colormap \n",
    "    cmap = colors.ListedColormap(['steelblue', '#a76c6e', '#6a9373'])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap, alpha=0.5, zorder=1)\n",
    "\n",
    "    ax.legend(loc=\"upper right\", fontsize=16)\n",
    "    ax.set_xlabel(\"sepal length (cm)\", fontsize=16)\n",
    "    ax.set_ylabel(\"sepal width (cm)\", fontsize=16)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Functions for Perceptron and Simulated Data \n",
    "# ----------------------------------------------------\n",
    "\n",
    "def data_plot(scatter=[], models=[]):\n",
    "    '''\n",
    "    Function to plot the dam data \n",
    "    '''\n",
    "    \n",
    "    # colors for scatter plots and model plots \n",
    "    scolors = [\"steelblue\", \"#a76c6e\", \"#6a9373\", \"orange\"]\n",
    "    mcolors = [\"black\", \"gray\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6,6))\n",
    "    \n",
    "    # Loop over scatter data and make plots \n",
    "    for ii, (x, y) in enumerate(scatter):\n",
    "        pos = x[y==1, :]\n",
    "        neg = x[y==-1, :]\n",
    "        ax.scatter(pos[:, 0], pos[:, 1], s=100, color=scolors[1], label=\"pos\", zorder=2)\n",
    "        ax.scatter(neg[:, 0], neg[:, 1], s=100, color=scolors[0], label=\"neg\", zorder=2)\n",
    "        \n",
    "    # Loop over model data and make plots \n",
    "    for ii, (xplot, yplot, label) in enumerate(models):\n",
    "        ax.plot(xplot, yplot, color=mcolors[ii], lw=3, label=label, zorder=1)\n",
    "        \n",
    "    # Set axis limits\n",
    "    ax.set_xlim([-1.5,1.5])\n",
    "    ax.set_ylim([-1.5,1.5])\n",
    "        \n",
    "    # Label all the things \n",
    "    ax.set_xlabel(r\"$x_1$\", fontsize=16)\n",
    "    ax.set_ylabel(r\"$x_2$\", fontsize=16)\n",
    "    ax.set_title(\"Data Plot\", fontsize=20)\n",
    "    ax.grid(alpha=0.25)\n",
    "    ax.legend(loc=\"upper left\", fontsize=12)\n",
    "    \n",
    "    return fig, ax \n",
    "\n",
    "def poly_data(n, mag=0, sep=0, rot=0.0, random_state=1235):\n",
    "    np.random.seed(random_state)\n",
    "    x1 = np.random.uniform(-1, 1, n)\n",
    "    x2 = np.random.uniform(-1, 1, n)\n",
    "    X = np.column_stack((x1, x2))\n",
    "    y = np.array([1 if mag * (x1i-1) * (x1i+1) * x1i < x2i else -1\n",
    "                  for x1i, x2i in zip(x1, x2)])\n",
    "    X[y==1, 1] += sep \n",
    "    X[y==-1, 1] -= sep \n",
    "    s = np.sin(rot)\n",
    "    c = np.cos(rot)\n",
    "    Q = np.array([[c,s], [-s, c]])\n",
    "    X = np.dot(X, Q)\n",
    "    shuffle = np.random.choice(range(X.shape[0]), replace=False, size=X.shape[0])\n",
    "    X, y = X[shuffle, :], y[shuffle]\n",
    "    return X, y\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
