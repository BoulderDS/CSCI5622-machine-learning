{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Neural Networks \n",
    "***\n",
    "\n",
    "In this notebook we'll implement a rudimentary Stochastic Gradient Descent algorithm with Back Propagation to learn the weights in a simple feed-forward neural network.  \n",
    "\n",
    "**Important Note**: We're basically going to implement 25% of your next homework assignment. I will post the solutions after today's class. Hope that will help with your homework.\n",
    "\n",
    "**Semi-Important Note**: There are some helper functions at the bottom of this notebook.  Scroll down and evaluate those before proceeding. \n",
    "\n",
    "A skeleton of our network class is given below.  We'll update it's functionality piece by piece as we work through the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n, 1) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m, n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros((n, 1)) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros((n, 1)) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros((n, 1)) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros((n, 1)) for n in self.sizes]\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def grad_loss(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = (a-y)^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "        # Initialize activation on initial layer to x \n",
    "        # TODO\n",
    "\n",
    "        # Loop over layers and compute activities and activations \n",
    "        # TODO \n",
    "        \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        \n",
    "        # forward prop training example to fill in activities and activations \n",
    "        # TODO\n",
    "        \n",
    "        # compute deltas on output layer \n",
    "        # TODO\n",
    "        \n",
    "        # loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        # TODO\n",
    "\n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None,\n",
    "              eta=0.25, num_epochs=10, isPrint=True, isVis=False):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded labels \n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                # back prop to get derivatives \n",
    "                # TODO \n",
    "                \n",
    "                # update weights and biases \n",
    "                # TODO\n",
    "                pass\n",
    "                \n",
    "            # print mean loss every 10 epochs if requested \n",
    "            if isPrint and (ep % 10) == 0:\n",
    "                print(\"epoch {:3d}/{:3d}: \".format(ep, num_epochs), end=\"\")\n",
    "                print(\"  train loss: {:8.3f}\".format(self.compute_loss(X_train, y_train)), end=\"\")\n",
    "                if X_valid is not None:\n",
    "                    print(\"  validation loss: {:8.3f}\".format(self.compute_loss(X_valid, y_valid)))\n",
    "                else:\n",
    "                    print(\"\")\n",
    "                    \n",
    "            if isVis and (ep % 20) == 0:\n",
    "                self.pretty_pictures(X_train, y_train, decision_boundary=True, epoch=ep)\n",
    "                    \n",
    "    def compute_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        compute average loss for given data set \n",
    "        \n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vector-encoded labels \n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        if len(X.shape) == 1:\n",
    "            X = X[np.newaxis, :]\n",
    "        if len(y.shape) == 1:\n",
    "            y = y[np.newaxis, :]\n",
    "        for x, t in zip(X, y):\n",
    "            self.forward_prop(x)\n",
    "            if len(t.shape) == 1:\n",
    "                t = t.reshape(-1, 1)\n",
    "            loss += 0.5 * np.sum((self.a[-1] - t) ** 2)\n",
    "        return loss / X.shape[0]\n",
    "    \n",
    "    \n",
    "    def gradient_check(self, x, y, h=1e-5):\n",
    "        \"\"\"\n",
    "        check whether the gradient is correct for X, y\n",
    "        \n",
    "        Assuming that back_prop has finished.\n",
    "        \"\"\"\n",
    "        for ll in range(self.L - 1):\n",
    "            oldW = self.W[ll].copy()\n",
    "            oldb = self.b[ll].copy()\n",
    "            for i in range(self.W[ll].shape[0]):\n",
    "                for j in range(self.W[ll].shape[1]):\n",
    "                    self.W[ll][i, j] = oldW[i, j] + h\n",
    "                    lxph = self.compute_loss(x, y)\n",
    "                    self.W[ll][i, j] = oldW[i, j] - h\n",
    "                    lxmh = self.compute_loss(x, y)\n",
    "                    grad = (lxph - lxmh) / (2 * h)\n",
    "                    assert abs(self.dW[ll][i, j] - grad) < 1e-5\n",
    "                    self.W[ll][i, j] = oldW[i, j]\n",
    "            for i in range(self.b[ll].shape[0]):\n",
    "                j = 0\n",
    "                self.b[ll][i, j] = oldb[i, j] + h\n",
    "                lxph = self.compute_loss(x, y)\n",
    "                self.b[ll][i, j] = oldb[i, j] - h\n",
    "                lxmh = self.compute_loss(x, y)\n",
    "                grad = (lxph - lxmh) / (2 * h)\n",
    "                assert abs(self.db[ll][i, j] - grad) < 1e-5\n",
    "                self.b[ll][i, j] = oldb[i, j]\n",
    "        \n",
    "        \n",
    "            \n",
    "    def pretty_pictures(self, X, y, decision_boundary=False, epoch=None):\n",
    "        \"\"\"\n",
    "        Function to plot data and neural net decision boundary\n",
    "        \n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vector-encoded labels \n",
    "        :param decision_boundary: whether or not to plot decision \n",
    "        :param epoch: epoch number for printing \n",
    "        \"\"\"\n",
    "        \n",
    "        mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\"}\n",
    "        colorlist = [c for (n,c) in mycolors.items()]\n",
    "        colors = [colorlist[np.argmax(yk)] for yk in y]\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "        \n",
    "        if decision_boundary:\n",
    "            xx, yy = np.meshgrid(np.linspace(-1.25,1.25,300), np.linspace(-1.25,1.25,300))\n",
    "            grid = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "            grid_pred = np.zeros_like(grid[:,0])\n",
    "            for ii in range(len(grid_pred)):\n",
    "                self.forward_prop(grid[ii,:])\n",
    "                grid_pred[ii] = np.argmax(self.a[-1])\n",
    "            grid_pred = grid_pred.reshape(xx.shape)\n",
    "            cmap = ListedColormap([\n",
    "                colorConverter.to_rgba('steelblue', alpha=0.30),\n",
    "                colorConverter.to_rgba('#a76c63', alpha=0.30)])\n",
    "            plt.contourf(xx, yy, grid_pred, cmap=cmap)\n",
    "            if epoch is not None: plt.text(-1.23,1.15, \"epoch = {:d}\".format(epoch), fontsize=16)\n",
    "\n",
    "        plt.scatter(X[:,0], X[:,1], color=colors, s=100, alpha=0.9)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Two-Dimensional Data \n",
    "***\n",
    "\n",
    "**Part A**: We'll be using our network to do binary classification of two-dimensional feature vectors.  Scroll down to the **Helper Functions** and examine the function ``generate_data``. Then mess around with the following cell to look at the various data sets available.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Network([2,3,2])\n",
    "X_train, y_train = generate_data(300, \"blobs\")\n",
    "nn.pretty_pictures(X_train, y_train, decision_boundary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Look at a few of the training examples, in particular pay attention to the way that the labels in ``y_train`` are encoded.  Why might we want to encode labels in this way? \n",
    "\n",
    "Note that for illustration purposes, we are using regression-style loss functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Implementing Forward Propagation \n",
    "***\n",
    "\n",
    "**Part A**: Go up to the ``__init__`` function in the ``Network Class``.  How are we initializing a network?  What data structures are we using to store things like weights, biases, deltas, etc? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the ``forward_prop`` function to implement forward propagation.  Your function should take in a single training example ``x`` and propagate it forward in the network, setting the activations and activities on the hidden and output layers.  Remember that the pseudocode that we wrote for forward-prop looked as follows: \n",
    "\n",
    "1. $\\quad$Initialize ${\\bf a}^1 = {\\bf x}$\n",
    "2. $\\quad$For $\\ell = 1, \\ldots, L-1$: \n",
    "3. $\\quad\\quad\\quad{\\bf z}^{\\ell+1} = W^\\ell {\\bf a}^\\ell + {\\bf b}^\\ell$\n",
    "3. $\\quad\\quad\\quad{\\bf a}^{\\ell+1} = g({\\bf z}^{\\ell+1})$\n",
    "\n",
    "**Note**: You'll have to adjust this method to deal with Python's zero-based indexing. \n",
    "\n",
    "When you think you're done, we can instantiate a ``Network`` with with $2$ neurons in the input layer, $3$ neurons in the sole hidden layer, and $2$ neurons in the output layer, and then forward prop one of the training examples. \n",
    "\n",
    "Check that your indexing was correct by making sure that all of the activations are now non-zero (remember, we initialized them to vectors of zeros). \n",
    "\n",
    "What other things could we check? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Network([2,3,2])\n",
    "nn.forward_prop(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Implementing Back Propagation \n",
    "***\n",
    "\n",
    "**Part A**: OK, now it's time to implement back propagation.  Complete the function ``back_prop`` in the ``Network`` class to use a single training example to compute the derivatives of the loss function with respect to the weights and the biases.  Remember, the pseudocode for back-prop was as follows: \n",
    "\n",
    "1. $\\quad$Forward propagate the training example ${\\bf x}$, ${\\bf y}$\n",
    "2. $\\quad$Compute the $\\delta^L = \\dfrac{\\partial \\mathscr{L}}{\\partial {\\bf a}^L} \\odot g'({\\bf z}^L)$\n",
    "3. $\\quad$For $\\ell = L-1, \\ldots, 1$: \n",
    "4. $\\quad\\quad\\quad \\dfrac{\\partial \\mathscr{L}}{\\partial W^\\ell} = \\delta^{\\ell+1} ({\\bf a}^\\ell)^T$\n",
    "5. $\\quad\\quad\\quad \\dfrac{\\partial \\mathscr{L}}{\\partial {\\bf b}^\\ell} = \\delta^{\\ell+1}$\n",
    "6. $\\quad\\quad\\quad\\delta^{\\ell} = (W^\\ell)^T\\delta^{\\ell+1} \\odot g'({\\bf z}^\\ell)$\n",
    "\n",
    "When you think you're done, instantiate a small ``Network`` and call back-prop for a single training example.  \n",
    "\n",
    "Check that it's likely working by checking that the derivative matrices ``dW`` and ``db`` are nonzero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Network([2,3,2])\n",
    "nn.back_prop(X_train[0,:], y_train[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.gradient_check(X_train[0, :], y_train[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Implementing Stochastic Gradient Descent \n",
    "***\n",
    "\n",
    "**Part A**: OK, now let's actually train a neural net!  Complete the missing code in ``train`` to loop over the training data in random order, call back-prop to get the derivatives, and then update the weights and the biases via SGD.  \n",
    "\n",
    "When you think you're done, execute the following code and watch the training loss evolve over the training process.  If you've done everything correctly, it'll hopefully go down!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_data(300, \"blobs\")\n",
    "X_valid, y_valid = generate_data(300, \"blobs\")\n",
    "\n",
    "nn = Network([2, 3, 2])\n",
    "nn.train(X_train, y_train, eta=0.25, num_epochs=101, isPrint=True, isVis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: OK!  If you think you've worked out the bugs, let's start looking at the results.  We'll build a simple neural network, train it on a training set, and watch the decision boundary of our classifier evolve to fit the data.  We can do this by running similar code as above, but with the `isVis` flag set to `True`. Note that producing the plots takes considerable computational work, so things will go a bit slower now.\n",
    "\n",
    "Start with the blobs data set, and then move on to more complicated data sets like `moons`, `circles`, and finally the `checkerboard`. Note that for these more complicated geometries, it'll probably be necessary to chain the number of neurons in your hidden layer, or even add more hidden layers!   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = generate_data(300, \"checkerboard\")\n",
    "X_valid, y_valid = generate_data(300, \"checkerboard\")\n",
    "\n",
    "nn = Network([2,100,100,2])\n",
    "nn.train(X_train, y_train, eta=0.25, num_epochs=101, isPrint=True, isVis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import colorConverter, ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "def generate_data(N, config=\"checkerboard\"):\n",
    "    X = np.zeros((N,2))\n",
    "    y = np.zeros((N,2)).astype(int)\n",
    "    \n",
    "    if config==\"checkerboard\":\n",
    "        nps, sqlen = N//9, 2/3\n",
    "        ctr = 0\n",
    "        for ii in range(3):\n",
    "            for jj in range(3):\n",
    "                X[ctr * nps : (ctr + 1) * nps, :] = np.column_stack(\n",
    "                    (np.random.uniform(ii * sqlen +.05-1, (ii+1) * sqlen - .05 -1, size=nps),\n",
    "                     np.random.uniform(jj * sqlen +.05-1, (jj+1) * sqlen - .05 -1, size=nps))) \n",
    "                y[ctr*nps:(ctr+1)*nps,(3*ii+jj)%2] = 1 \n",
    "                ctr += 1\n",
    "                \n",
    "    if config==\"blobs\":            \n",
    "        X, yflat = datasets.make_blobs(n_samples=N, centers=[[-.5,.5],[.5,-.5]],\n",
    "                                       cluster_std=[.20,.20],n_features=2)\n",
    "        for kk, yk in enumerate(yflat):\n",
    "            y[kk,:] = np.array([1,0]) if yk else np.array([0,1])\n",
    "            \n",
    "    \n",
    "    if config==\"circles\":\n",
    "        kk=0\n",
    "        while kk < N / 2:\n",
    "            sample = 2 * np.random.rand(2) - 1 \n",
    "            if np.linalg.norm(sample) <= .45:\n",
    "                X[kk,:] = sample \n",
    "                y[kk,:] = np.array([1,0])\n",
    "                kk += 1 \n",
    "        while kk < N:\n",
    "            sample = 2 * np.random.rand(2) - 1\n",
    "            dist = np.linalg.norm(sample)\n",
    "            if dist < 0.9 and dist > 0.55:\n",
    "                X[kk,:] = sample \n",
    "                y[kk,:] = np.array([0,1])\n",
    "                kk += 1\n",
    "                \n",
    "    if config==\"moons\":\n",
    "        X, yflat = datasets.make_moons(n_samples=N, noise=.05)\n",
    "        X[:,0] = .5 * (X[:,0] - .5)\n",
    "        X[:,1] = X[:,1] - .25\n",
    "        for kk, yk in enumerate(yflat):\n",
    "            y[kk, :] = np.array([1,0]) if yk else np.array([0,1])\n",
    "            \n",
    "    return X, y\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
